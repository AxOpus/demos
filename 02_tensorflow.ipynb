{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "assert sys.version_info.major == 3\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - TensorFlow\n",
    "### 2.1 - Overview\n",
    "\n",
    "Now we're going to introduce the TensorFlow (TF) framework. This is very much using a sledgehammer to crack a nut, but introducing it in this simple example provides an nice opportunity to explain it. This introduction is largely taken from the [official](https://www.tensorflow.org/get_started/get_started) getting started guide.\n",
    "\n",
    "##### 2.1.1 - Tensors\n",
    "TF takes its name from the **tensor** which is a data structure of primative values, which are shaped into an array:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "3 # a rank 0 tensor; this is a scalar with shape []\n",
    "[1., 2., 3.] # a rank 1 tensor; this is a vector with shape [3]\n",
    "[[1., 2., 3.], [4., 5., 6.]] # a rank 2 tensor; a matrix with shape [2, 3]\n",
    "[[[1., 2., 3.]], [[7., 8., 9.]]] # a rank 3 tensor with shape [2, 1, 3]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think. What would a tensor of shape [1, 2, 3, 4] look like? \n",
    "\n",
    "Hint: the dimensions are the same as in numpy, so you can play around with that to get a feel for higher-dimensional matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "(2, 1, 3)\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1., 2., 3.], [4., 5., 6.]])\n",
    "b = np.array([[[1., 2., 3.]], [[7., 8., 9.]]])\n",
    "print(a.shape)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's hard to visualise a 4D-tensor, however they are useful in real-world problems such as image classification.\n",
    "\n",
    "The computer 'sees' an image as a matrix of values, where each value represents one pixel. So imagine a 2D image of 20px $\\times$ 20px, this is Tensor of shape [20, 20]. Now imagine we introduce Red, Green, and Blue colour channels. We can represent each channel as a separate matrix of 20px $\\times$ 20px, and we can stack these onto each other to produce a Tensor of shape [20, 20, 3].\n",
    "\n",
    "Now, imagine we have a batch of 50 images. Well, we can stack these again to form a Tensor of shape [50, 32, 32, 3]. Now suppose there are 10 batches of 50 images. Could we stack these? Sure, and we would have a Tensor of shape [10, 50, 32, 32, 3].\n",
    "\n",
    "Hopefully this helps in visualising past 3 dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 - TF structure\n",
    "TF is a framework which is designed to facilitate the development of deep learning, and can be thought of as creating a a program in two parts:\n",
    "\n",
    "1. Building the computational graph\n",
    "2. Running the computational graph\n",
    "\n",
    "A computational graph can be though of as a series of **nodes** which take in 0 or more tensors and output a tensor. This could be a node which creates a constant tensor (takes in 0 tensors and outputs 1 tensor), or a node which multiplies tensors together (takes in $n$ tensors, and outputs 1 tensor). In the latter example, these are known as **operations**.\n",
    "\n",
    "Think of the computational graph as being like a series of pipes, that split and rejoin at various points. Running the computational graph would be like pouring water into the pipes, and seeing it flow around the pipework. Although instead of water, we have data.\n",
    "\n",
    "Let's start with some simple examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Getting started\n",
    "\n",
    "Let's build a node which creates a constant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "node_1 = tf.constant(3.0, dtype=tf.float32, name='node_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That seems like a lot of code to produce a constant floating point number, and it is, especially if you are used to Python. However it helps to construct the graph, since it is very explicit. From the docs:\n",
    "```\n",
    "tf.constant(value, dtype=None, shape=None, name='Const', verify_shape=False)\n",
    "```\n",
    "\n",
    "So we can see that there is a slightly more compact way to create the node, if we wanted:\n",
    "\n",
    "```\n",
    "node_1 = tf.constant(3.0)\n",
    "```\n",
    "\n",
    "However, at this stage it helps to get into the habit of being explicit, _especially_ with naming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens when we print this node? Should we see 3.0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"node_1:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(node_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, that's odd. We've got a tensor, with the name of the node, but not much else. This is because we have only completed the first part of the program: building the computational graph. We still need to run the graph.\n",
    "\n",
    "To do this, we need to first create a session. This is an object which encapsulates the environment in which operations are exececuted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use this object's ```run``` method to execute our node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(node_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to close our session, to avoid using resources that may be required by other sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of this, it is more common to construct sessions using the ```with``` statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(sess.run(node_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This automatically closes the session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Operations\n",
    "\n",
    "Now we are going to build a very simple graph, which adds two numbers together. \n",
    "\n",
    "First create our constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "node_1 = tf.constant(3.0, dtype=tf.float32, name='node_1')\n",
    "node_2 = tf.constant(5.0, dtype=tf.float32, name='node_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to create a node to add these two nodes together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "node_3 = tf.add(node_1, node_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, we have just built the graph at the moment. Now, let's run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(sess.run(node_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first (proper) graph works! At this stage it may not be clear exactly why we need all this code to add two numbers together, but hopefully it will become clearer shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# OPTIONAL\n",
    "# Multiply the two numbers together\n",
    "\n",
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Placeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So far we've seen how to create constants, but you may be thinking wondering about the analogy from above regarding pouring water into the pipes. To do this 'pouring' we need to define **placeholders**.\n",
    "\n",
    "These are nodes which accept external values, which are fed (or poured) when we run the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "node_1 = tf.placeholder(dtype=tf.float32, name='node_1')\n",
    "node_2 = tf.placeholder(dtype=tf.float32, name='node_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, they follow a similar syntax as the constants, however they do not have a value associated with them yet.\n",
    "\n",
    "To give these nodes values, we need to supply something called a **feed dictionary**, which is a Python dictionary mapping the nodes to the input values.\n",
    "\n",
    "We'll present the example from above, where we add the two nodes together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0\n"
     ]
    }
   ],
   "source": [
    "node_3 = tf.add(node_1, node_2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(node_3, feed_dict={node_1: 3.0, node_2: 5.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the features of the feed dict is that we can supply lists. This allows us to feed in multiple values to our operation node. For example, let's add 3.0 to 5.0, and 1.0 to 4.0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8.  5.]\n"
     ]
    }
   ],
   "source": [
    "node_3 = tf.add(node_1, node_2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(node_3, feed_dict={node_1: [3.0, 1.0], node_2: [5.0, 4.0]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We've now built a computational graph, supplied data to it, and ran it. We're closing in on having a solid understanding of TensorFlow now. But before we move on to using it for linear regression, there's one more important node to be aware of."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 - Variables\n",
    "\n",
    "A variable is a TF node which, as its name suggests, can vary. If you remember our $w_1$ value from before and how it changed during gradient descent, this is a prime example of a use of variables.\n",
    "\n",
    "Initialisation is (somewhat) similar to our previous nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # This is required if we run this cell multiple times, \n",
    "                         # otherwise we will try and create the variable repeatedly.\n",
    "                         # Try commenting this command and running the cell twice.\n",
    "        \n",
    "node_4 = tf.get_variable('node_4', shape=[],  dtype=tf.float32, \n",
    "                         initializer=tf.contrib.layers.xavier_initializer()\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the syntax is similar, with a couple of additional values to specify.\n",
    "\n",
    "First, we need to supply a shape for the Tensor, which is straightforward. We also define an initializer, which if you recall from the previous notebook, allows us to set an initial value not equal to 0. The specifics of this particular initialiser are beyond the scope of this notebook, but if you would like to read more [this](http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization) is a good start.\n",
    "\n",
    "Furthermore, there is also a tf.Variable node which is more heavily used in the documentation, but less so in practice. Some reasons why are discussed [here](https://stackoverflow.com/questions/37098546/difference-between-variable-and-get-variable-in-tensorflow).\n",
    "\n",
    "An example of using the placeholder is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer() # This is a required command which initialises our variables.\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init) # Run the initialisation.\n",
    "    x = 3\n",
    "    print(sess.run(tf.assign(node_4, x))) # This should assign the value of x to our Variable node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's our initial overview of TensorFlow complete. It is meant to show you the syntax and get you comfortable with the idea of building a computational graph.\n",
    "\n",
    "In the next notebook we will revisit linear regression and tackle it using TensorFlow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
